<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">


    <script src="https://kit.fontawesome.com/f49e6ed101.js" crossorigin="anonymous"></script>
    <link href='https://fonts.googleapis.com/css?family=Rubik' rel='stylesheet'>
    <link rel="stylesheet" href="styles.css">
    <title>Super Resolution Research</title>

</head>
<body>
    <!-- Main Page Header -->
    <div class="page_header" id="sticky">
        <table class="header_table">
            <tr class="header_table_row">
                <td>
                    <!-- logo -->
                    <i class="fa-solid fa-expand fa-2x"></i>
                </td>
                <td class="header_table_cell_1">
                    <span class="header-title"><a href="#">Super-Resolution</a></span>
                    <br> <!-- go to new line -->
                    <span class="header-title2">Texture Detail Retention</span>
                </td>
                <td class="header_table_cell_2">
                    <ul class="nav-bar">
                        <li><a href="#seth">Seth Klupka</a></li>
                        <li><a href="#robert">Robert Thomas</a></li>
                        <li><a href="#krutin">Krutin Patel</a></li>
                        <li><a href="#aisha">Aisha Bah</a></li>
                        <li><a href="#hannah">Hannah Bradley</a></li>
                    </ul>  
                </td>
            </tr>
        </table>
    </div>



    <!-------------------->
    <!--    SFT GAN         seth -->
    <!-------------------->
    <div class="section-super-header" id="seth">SFT - GAN</div>
    <div class="section-super-paragraph">Special Feature Transform - Generative Adversarial Network</div>
    <div class="section-super-author-name">Seth Klupka</div>
    <!-- Central Page Div -->
    <div class="center-content-div">
        <h2 class="section-header">Abstract</h2>
        <p class="section-paragraph" id="abstract">Despite that convolutional neural networks (CNN) have recently demonstrated high-quality reconstruction for single-image super-resolution (SR), recovering natural and realistic texture remains a challenging problem. In this paper, we show that it is possible to recover textures faithful to semantic classes . In particular, we only need to modulate features of a few intermediate layers in a single network conditioned on semantic segmentation probability maps. This is made possible through a novel Spatial Feature Transform (SFT) layer that generates affine transformation parameters for spatial-wise feature modulation. SFT layers can be trained end-to-end together with the SR network using the same loss function. During testing, it accepts an input image of arbitrary size and generates a high-resolution image with just a single forward pass conditioned on the categorical priors. Our final results show that an SR network equipped with SFT can generate more realistic and visually pleasing textures in comparison to state-of-the-art SRGAN and EnhanceNet.</p>
        <h2 class="section-header">Introduction</h2>
        <p class="section-paragraph">SFT-GAN stands for Special Feature Transform which is a layer proposed to efficiently incorporate the categorical conditions into a CNN network. A super resolution network equipped with SFT layers can generate richer texture detail, which is the goal of this paper. The figure below shows the difference in detail generated by a state-of-the-art method and the new SFT-GAN method. Notice that the new method has much less distortion and retains the brick texture.</p>
        <img class="section-img" src="images\SOTA-vs-SFTGAN.png" alt="SFT-GAN image">
        <p class="section-paragraph">To achieve better texture details, we must overcome a large problem. That being able to understand the texture context given two low resolution images. In the figure below, two patches are examined. One containing bricks, and the other being a flower bush. At low resolution, these patches look very similar and are difficult to determine what is being depicted.</p>
        <img class="section-img" src="images\lr-patches.png" alt="SFT-GAN image">

        <h2 class="section-header">Problem Solving</h2>
        <p class="section-paragraph">To solve this context issue, semantic segmentation maps are employed. These maps are made from classified regions in an image. Regions that identify buildings, plants, and or the sky. The figure below shows these maps and how each subject is accurately classified into segments, no matter the resolution.</p>
        <img class="section-img" src="images\ssm.png" alt="SFT-GAN image">
        <p class="section-paragraph">SFT layers are conditioned on semantic segmentation probability maps, based on which it generates a pair of modulation parameters to apply affine transformation spatially on feature maps of the network.</p>
    
        <h2 class="section-header">SFT Layer Integration and Architecture</h2>
        <p class="section-paragraph">This figure below shows how SFT layers can be implemented in a super resolution network. All SFT layers share a condition network. The role of the condition network is to generate intermediate conditions from the prior, and broadcast the conditions to all SFT layers for further generation of modulation parameters. Some advantage of SFT layers are that they are parameter efficient, can easily be introduced to existing networks, they provide rich semantic regions in just a single forward pass, and also can be used in conjunction with depth maps.</p>
        <img class="section-img" src="images\SFT-arch.png" alt="SFT-GAN image">
        <p class="section-paragraph">In a typical super resolution study, images are downsampled by 4 times which is used as input shown in the top left. Using a 4x downsample still produces satisfactory segmentation results. The architecture is split into two streams: a condition network and a super resolution network. The condition network takes segmentation probability maps as input, shown at the bottom left, and are processed by 4 convolution layers. These will generate intermediate conditions shared by the SFT layers. The super resolution network is built with 16 residual blocks with the SFT layers, which take the shared conditions as input and learn to modulate the feature maps.</p>

        <h2 class="section-header">Results and Conclusion</h2>
        <p class="section-paragraph">Pretrained models were required for execution and only the supplied test images work. During testing, the program took the high resolution images seen on the left of the figure below and downsampled them by 4 times. These low resolution images are used as input for both the semantic segmentation maps and the network. Next the semantic segmentation maps are created and then provided as input for SFT-GAN. From there the network will output the final images. As you can see on the right of the figure below, the output images are of much higher resolution when compared to the 4x downsamples. Comparing the output to the original images, you will notice that the texture of the bricks and fur are present. Although there is still some distortion seen in the house image, most notably the window, the texture of the bricks are very much apparent and this paper achieves what it set out to accomplish.</p>
        <img class="section-img" src="images\sft-results.png" alt="SFT-GAN image">
    </div>



    <!-------------------->
    <!--   IDN-Caffe        robert -->
    <!-------------------->
    <div class="section-super-header" id="robert">IDN-Caffe</div>
    <div class="section-super-paragraph">Information Distillation Network - Caffe</div>
    <div class="section-super-author-name">Robert Thomas</div>
    <!-- Central Page Div -->
    <div class="center-content-div">
        <h2 class="section-header">Introduction</h2>
        <p class="section-paragraph">The IDN model, or information distillation network, was developed in order to address the issue faced by many models that rely on CNNs. That is, in order to create higher quality images, deeper and wider networks are needed by the model. This leads to an overall higher cost in computational power by the system. The IDN model is designed with lightweight parameters and computational complexity in order to address this.</p>
        <h2 class="section-header">Architecture</h2>
        <img class="section-img" src="images\robert-arch.png" alt="SFT-GAN image">
        <p class="section-paragraph">The IDN consists of 3 major parts: a feature extraction block, multiple stacked distillation blocks, and a reconstruction block. In the FBlock, two 3x3 convolutional layers are used to extract the feature maps from the original image. Each DBlock contains an enhancement unit and a compression unit with stacked style. Each individual DBlock is then chained together with the next for as many times as desired. The output from these blocks is then fed into the RBlock for reconstruction. The output from the RBlock is then combined with the bicubic output to create the new image.</p>
        <h2 class="section-header">Results and Conclusion</h2>
        <img class="section-img" src="images\robert-results.png" alt="SFT-GAN image">
        <p class="section-paragraph">In these examples we see the results of the model. There is a minor increase in quality from the original image, and a major increase from the bicubic image. In the image with the bird it is more pronounced, with the stripes on the bird being more defined and focused from the original image. Overall, while the IDN model does show a marked increase in quality over the bicubic model, it only has a minor increase over the original image. In some cases, it would be fair to conclude that the image is actually of a lower quality compared to the original image.</p>
    </div>


    <!-------------------->
    <!--     SRGAN          krutin --> 
    <!-------------------->
    <div class="section-super-header" id="krutin">SRGAN</div>
    <div class="section-super-paragraph">Super-Resolution Generative Adversarial Network</div>
    <div class="section-super-author-name">Krutin Patel</div>
    <!-- Central Page Div -->
    <div class="center-content-div">
        <h2 class="section-header">Introduction</h2>
        <p class="section-paragraph">Goal: Use the SRGAN model to create SR pictures from low-resolution photos or videos.<br><br>Problem: Finer texture features can be recovered when upscaling factors are high, although this could leave low-resolution images missing high-frequency details.</p>
        <p class="section-paragraph">Estimating the differences and alignment between high-resolution and low-resolution images from their corresponding super-resolution is a difficult task, and an underappreciated SR difficulty is the high degree of upscaling factors and well-rebuilt texture features.</p>
        <h2 class="section-header">Architecture</h2>
        <img class="section-img" src="images\krutin_arch.png" alt="SRGAN image">
        <p class="section-paragraph">Generator network utilizing fully convolutional SRRESNET model architecture and high-quality SR images.<br><br>The discriminator model assures the overall architecture's quality of the images, the discriminator model functions as a model classifier and produces natural images. The ReLu layer is followed by a 9x9 convolutional layer with 64 features in the generator architecture. ReLu is used because of the non-linear functions of the mapping of LR to HR pictures. The following phase makes use of residual blocks, which have a batch normalization layer after them and a CNN layer of 3x3 kernels and 64 features. Discriminator architecture supports a normal GAN technique, and when a generator and discriminator are combined simultaneously, DA can locate phony images while GA attempts to create realistic images while avoiding discriminator detection.</p>
        <h2 class="section-header">Conclusion</h2>
        <img class="section-img" src="images\krutin_conc.png" alt="SRGAN image">
        <p class="section-paragraph">The reconstruction results and relevant references for the HR pictures using 4x upscaling are shown in the figure.</p>
        <img class="section-img" src="images\krutin_conc2.png" alt="SRGAN image">
        <p class="section-paragraph">In the table, it is the comparison of NN, bicubic, SRGAN, and various models including original HR on benchmark data with 4x upscaling.</p>
        <h2 class="section-header">Summary</h2>
        <p class="section-paragraph">A proposed SRGAN model that uses a loss function that incorporates adversarial loss and content loss uses a loss function to recover finer texture, however, may leave LR pictures lacking in frequency details. In order to produce better texture details, the Adversarial Loss pushes the solution into natural images utilizing the discriminator network that is trained between the SR images and the original photo-realistic images.</p>
        <h2 class="section-header">Resources</h2>
        <p class="section-paragraph">Github: https://github.com/idealo/image-super-resolution<br><br>Paper: https://openaccess.thecvf.com/content_cvpr_2017/papers/Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper.pdf<br><br>Authors: Christian Ledig, Lucas Theis, Ferenc Huszar, Jose Caballero, Andrew Cunningham, Alejandro Acosta, Andrew Aitken, Alyken Tejani, Johannes Totz, Zehan Wang, Wenzhe Shi Twitter</p>
    </div>



    <!-------------------->
    <!--     EDSR           aisha -->
    <!-------------------->
    <div class="section-super-header" id="aisha">EDSR</div>
    <div class="section-super-paragraph">Enhanced Deep Super Resolution</div>
    <div class="section-super-author-name">Aisha Bah</div>
    <!-- Central Page Div -->
    <div class="center-content-div">
    </div>



    <!-------------------->
    <!--     SRNTT          hannah -->
    <!-------------------->
    <div class="section-super-header" id="hannah">SRNTT</div>
    <div class="section-super-paragraph">Super-Resolution by Neural Texture Transfer</div>
    <div class="section-super-author-name">Hannah Bradley</div>
    <!-- Central Page Div -->
    <div class="center-content-div">
        <h2 class="section-header">Introduction</h2>
        <p class="section-paragraph">SRNTT(Super-Resolution by Neural Texture Transfer), is a reference-based super-resolution approach which takes an input reference image and low resolution image as input. The main idea of this approach is to adaptively transfer textures from the reference image to the low resolution one.</p>
        <img class="section-img" src="images\hannah-intro.png" alt="SFT-GAN image">
        <h2 class="section-header">Architecture</h2>
        <p class="section-paragraph">The main things to note about this model is the use of feature swapping and texture transfer. What it’s supposed to do is search for any matching texture from the reference photo in the feature space and then transfer any of the matching textures to the super resolution image in a multi-scale fashion.</p>
        <img class="section-img" src="images\hannah-arch.png" alt="SFT-GAN image">
        <p class="section-paragraph">The model makes sure to regularize on the texture consistency between the super resolution image and the matched textures of the reference image. The final output is synthesized in an end-to-end manner.</p>
        <h2 class="section-header">Architecture Texture Transfer</h2>
        <p class="section-paragraph">One of the most important structures of SRNTT is the texture transfer aspect. This figure shows an illustration of what the network structure of texture transfer looks like at a one scale, where the residual blocks extract related texture from the reference image based on the low resolution image and merge it with target content.</p>
        <img class="section-img" src="images\hannah-arch2.png" alt="SFT-GAN image">
        <p class="section-paragraph">The textures that are dissimilar to the low resolution image will have lower weight, so in the texture transfer there will be a lower penalty for it. This means the texture transfer from reference image to the super resolution image is adaptively enforced based on the quality of the reference image, leading to more robust texture hallucination.</p>
        <h2 class="section-header">Conclusion</h2>
        <p class="section-paragraph">SRNTT is best known to have good performance when the low resolution image and reference image are misaligned to a certain extent. SRNTT can restore better texture details, but the final reconstruction result is not done perfectly due to the low utilization of features between channels.</p>
    </div>


    <!-- FOOTER -->
    <div id="footer">
        @ 2022 University of Texas at San Antonio, All Rights Reserved.<br>
        Seth Klupka, Robert Thomas, Krutin Patel, Aisha Bah, Hannah Bradley.
    </div>
</body>
</html>

