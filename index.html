<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">


    <script src="https://kit.fontawesome.com/f49e6ed101.js" crossorigin="anonymous"></script>
    <link href='https://fonts.googleapis.com/css?family=Rubik' rel='stylesheet'>
    <link rel="stylesheet" href="styles.css">
    <title>Super Resolution Research</title>

</head>
<body>
    <!-- Main Page Header -->
    <div class="page_header" id="sticky">
        <table class="header_table">
            <tr class="header_table_row">
                <td>
                    <!-- logo -->
                    <i class="fa-solid fa-expand fa-3x"></i>
                </td>
                <td class="header_table_cell_1">
                    <span class="header-title"><a href="#">Super-Resolution</a></span>
                    <br> <!-- go to new line -->
                    <span class="header-title2">Texture Detail Retention</span>
                </td>
                <td class="header_table_cell_2">
                    <ul class="nav-bar">
                        <li><a href="#seth">Seth Klupka</a></li>
                        <li><a href="#hannah">Hannah Bradley</a></li>
                        <li><a href="#robert">Robert Thomas</a></li>
                        <li><a href="#krutin">Krutin Patel</a></li>
                        <li><a href="#aisha">Aisha Bah</a></li>
                    </ul>  
                </td>
            </tr>
        </table>
    </div>



    <!-------------------->
    <!--    SFT GAN         seth -->
    <!-------------------->
    <div class="section-super-header" id="seth">SFT - GAN</div>
    <div class="section-super-paragraph">Special Feature Transform - Generative Adversarial Network</div>
    <div class="section-super-author-name">Seth Klupka</div>
    <!-- Central Page Div -->
    <div class="center-content-div">
        <h2 class="section-header">Abstract</h2>
        <p class="section-paragraph" id="abstract">Despite that convolutional neural networks (CNN) have recently demonstrated high-quality reconstruction for single-image super-resolution (SR), recovering natural and realistic texture remains a challenging problem. In this paper, we show that it is possible to recover textures faithful to semantic classes . In particular, we only need to modulate features of a few intermediate layers in a single network conditioned on semantic segmentation probability maps. This is made possible through a novel Spatial Feature Transform (SFT) layer that generates affine transformation parameters for spatial-wise feature modulation. SFT layers can be trained end-to-end together with the SR network using the same loss function. During testing, it accepts an input image of arbitrary size and generates a high-resolution image with just a single forward pass conditioned on the categorical priors. Our final results show that an SR network equipped with SFT can generate more realistic and visually pleasing textures in comparison to state-of-the-art SRGAN and EnhanceNet.</p>
        <h2 class="section-header">Introduction</h2>
        <p class="section-paragraph">SFT-GAN stands for Special Feature Transform which is a layer proposed to efficiently incorporate the categorical conditions into a CNN network. A super resolution network equipped with SFT layers can generate richer texture detail, which is the goal of this paper. The figure below shows the difference in detail generated by a state-of-the-art method and the new SFT-GAN method. Notice that the new method has much less distortion and retains the brick texture.</p>
        <img class="section-img" src="images\SOTA-vs-SFTGAN.png" alt="SFT-GAN image">
        <p class="section-paragraph">To achieve better texture details, we must overcome a large problem. That being able to understand the texture context given two low resolution images. In the figure below, two patches are examined. One containing bricks, and the other being a flower bush. At low resolution, these patches look very similar and are difficult to determine what is being depicted.</p>
        <img class="section-img" src="images\lr-patches.png" alt="SFT-GAN image">

        <h2 class="section-header">Problem Solving</h2>
        <p class="section-paragraph">To solve this context issue, semantic segmentation maps are employed. These maps are made from classified regions in an image. Regions that identify buildings, plants, and or the sky. The figure below shows these maps and how each subject is accurately classified into segments, no matter the resolution.</p>
        <img class="section-img" src="images\ssm.png" alt="SFT-GAN image">
        <p class="section-paragraph">SFT layers are conditioned on semantic segmentation probability maps, based on which it generates a pair of modulation parameters to apply affine transformation spatially on feature maps of the network.</p>
    
        <h2 class="section-header">SFT Layer Integration and Architecture</h2>
        <p class="section-paragraph">This figure below shows how SFT layers can be implemented in a super resolution network. All SFT layers share a condition network. The role of the condition network is to generate intermediate conditions from the prior, and broadcast the conditions to all SFT layers for further generation of modulation parameters. Some advantage of SFT layers are that they are parameter efficient, can easily be introduced to existing networks, they provide rich semantic regions in just a single forward pass, and also can be used in conjunction with depth maps.</p>
        <img class="section-img" src="images\SFT-arch.png" alt="SFT-GAN image">
        <p class="section-paragraph">In a typical super resolution study, images are downsampled by 4 times which is used as input shown in the top left. Using a 4x downsample still produces satisfactory segmentation results. The architecture is split into two streams: a condition network and a super resolution network. The condition network takes segmentation probability maps as input, shown at the bottom left, and are processed by 4 convolution layers. These will generate intermediate conditions shared by the SFT layers. The super resolution network is built with 16 residual blocks with the SFT layers, which take the shared conditions as input and learn to modulate the feature maps.</p>

        <h2 class="section-header">Results and Conclusion</h2>
        <p class="section-paragraph">Pretrained models were required for execution and only the supplied test images work. During testing, the program took the high resolution images seen on the left of the figure below and downsampled them by 4 times. These low resolution images are used as input for both the semantic segmentation maps and the network. Next the semantic segmentation maps are created and then provided as input for SFT-GAN. From there the network will output the final images. As you can see on the right of the figure below, the output images are of much higher resolution when compared to the 4x downsamples. Comparing the output to the original images, you will notice that the texture of the bricks and fur are present. Although there is still some distortion seen in the house image, most notably the window, the texture of the bricks are very much apparent and this paper achieves what it set out to accomplish.</p>
        <img class="section-img" src="images\sft-results.png" alt="SFT-GAN image">
    </div>

    <!-------------------->
    <!--     SRNTT          hannah -->
    <!-------------------->
    <div class="section-super-header" id="hannah">SRNTT</div>
    <div class="section-super-paragraph">Super-Resolution by Neural Texture Transfer</div>
    <div class="section-super-author-name">Hannah Bradley</div>
    <!-- Central Page Div -->
    <div class="center-content-div">
    </div>



    <!-------------------->
    <!--   IDN-Caffe        robert -->
    <!-------------------->
    <div class="section-super-header" id="robert">IDN-Caffe</div>
    <div class="section-super-paragraph">Information Distillation Network - Caffe</div>
    <div class="section-super-author-name">Robert Thomas</div>
    <!-- Central Page Div -->
    <div class="center-content-div">
    </div>


    <!-------------------->
    <!--     SRGAN          krutin --> 
    <!-------------------->
    <div class="section-super-header" id="krutin">SRGAN</div>
    <div class="section-super-paragraph">Super-Resolution Generative Adversarial Network</div>
    <div class="section-super-author-name">Krutin Patel</div>
    <!-- Central Page Div -->
    <div class="center-content-div">
        <h2 class="section-header">Introduction</h2>
        <p class="section-paragraph">Goal: Use the SRGAN model to create SR pictures from low-resolution photos or videos.<br><br>Problem: Finer texture features can be recovered when upscaling factors are high, although this could leave low-resolution images missing high-frequency details.</p>
        <p class="section-paragraph">Estimating the differences and alignment between high-resolution and low-resolution images from their corresponding super-resolution is a difficult task, and an underappreciated SR difficulty is the high degree of upscaling factors and well-rebuilt texture features.</p>
        <h2 class="section-header">Architecture</h2>
        <img class="section-img" src="images\krutin_arch.png" alt="SRGAN image">
        <p class="section-paragraph">Generator network utilizing fully convolutional SRRESNET model architecture and high-quality SR images.<br><br>The discriminator model assures the overall architecture's quality of the images, the discriminator model functions as a model classifier and produces natural images. The ReLu layer is followed by a 9x9 convolutional layer with 64 features in the generator architecture. ReLu is used because of the non-linear functions of the mapping of LR to HR pictures. The following phase makes use of residual blocks, which have a batch normalization layer after them and a CNN layer of 3x3 kernels and 64 features. Discriminator architecture supports a normal GAN technique, and when a generator and discriminator are combined simultaneously, DA can locate phony images while GA attempts to create realistic images while avoiding discriminator detection.</p>
        <h2 class="section-header">Conclusion</h2>
        <img class="section-img" src="images\krutin_conc.png" alt="SRGAN image">
        <p class="section-paragraph">The reconstruction results and relevant references for the HR pictures using 4x upscaling are shown in the figure.</p>
        <img class="section-img" src="images\krutin_conc2.png" alt="SRGAN image">
        <p class="section-paragraph">In the table, it is the comparison of NN, bicubic, SRGAN, and various models including original HR on benchmark data with 4x upscaling.</p>
        <h2 class="section-header">Summary</h2>
        <p class="section-paragraph">A proposed SRGAN model that uses a loss function that incorporates adversarial loss and content loss uses a loss function to recover finer texture, however, may leave LR pictures lacking in frequency details. In order to produce better texture details, the Adversarial Loss pushes the solution into natural images utilizing the discriminator network that is trained between the SR images and the original photo-realistic images.</p>
        <h2 class="section-header">Resources</h2>
        <p class="section-paragraph">Github: https://github.com/idealo/image-super-resolution<br><br>Paper: https://openaccess.thecvf.com/content_cvpr_2017/papers/Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper.pdf<br><br>Authors: Christian Ledig, Lucas Theis, Ferenc Huszar, Jose Caballero, Andrew Cunningham, Alejandro Acosta, Andrew Aitken, Alyken Tejani, Johannes Totz, Zehan Wang, Wenzhe Shi Twitter</p>
    </div>



    <!-------------------->
    <!--     EDSR           aisha -->
    <!-------------------->
    <div class="section-super-header" id="aisha">EDSR</div>
    <div class="section-super-paragraph">Enhanced Deep Super Resolution</div>
    <div class="section-super-author-name">Aisha Bah</div>
    <!-- Central Page Div -->
    <div class="center-content-div">
    </div>



</body>
</html>

